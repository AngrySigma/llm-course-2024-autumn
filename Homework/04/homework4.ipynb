{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8275988",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Реализация MinHashLSH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852af4b",
   "metadata": {},
   "source": [
    "## Алгоритм MinHash. https://en.wikipedia.org/wiki/MinHash\n",
    "\n",
    "1) Разбиваем текст на токены/шинглы \\\n",
    "2) Берем хеш функцию от каждого шингла \\\n",
    "3) Выбираем для каждого документа первый минимальный хеш (не пустой) и индекс этого элемента записываем в матрицу сигнатур \\\n",
    "4) Получаем матрицу сигнатур: премешиваем строки -> первый минимальный хеш может измениться -> выбираем (повторяем это N раз) \\\n",
    "5) Считаем расстояние жаккарда по полученной матрице"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c456338",
   "metadata": {},
   "source": [
    "## пример:\n",
    "\n",
    "возьмем 3 документа: \n",
    "- Doc1 = 'я люблю Париж' \n",
    "- Doc2 = 'я не люблю Париж' \n",
    "- Doc3 = 'я люблю Рим' \n",
    "\n",
    "\n",
    "1) посплитим их на слова (тут можно усложнять, брать шинглы, обученный токенезатор и т.д.) \\\n",
    "\n",
    "- Doc1 = ['я', 'люблю', 'Париж'] \n",
    "- Doc2 = ['я', 'не люблю', 'Париж'] \n",
    "- Doc3 = ['я', 'люблю', 'Рим']\n",
    "\n",
    "2) составим матрицу встречаемости каждого из слов\n",
    "\n",
    "картинка\n",
    "\n",
    "\n",
    "3) Перемешаем индексы (чтобы перемешивать - можно использовать хеш функцию, которую применяем к индексу)\n",
    "\n",
    "картинка\n",
    "\n",
    "для первого перемешивания: \\\n",
    "[3, 1, 2] \\\n",
    "для второго перемешивания: \\\n",
    "[2, 1, 2] \\\n",
    "для третьего: \\\n",
    "[2, 1, 3]\n",
    "\n",
    "\n",
    "4) получаем матрицу сигнатур \\\n",
    "    Doc1 Doc2 Doc3  \\\n",
    "    [3,   1,    2]  \\\n",
    "    [2,   1,    2]  \\\n",
    "    [2,   1,    3] \n",
    "\n",
    "5) Расстояние Жаккарда:\n",
    "\n",
    "          Doc1 Doc2 Doc3 \n",
    "    Doc1    1    0   1/2 \n",
    "    Doc2    0    1    0  \n",
    "    Doc3    1/2  0    1  \n",
    "    \n",
    "    \n",
    "Получаем, что 1 и 3 документ хоть как-то похожи, а больше похожих пар нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a27f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44427a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2a2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('train-00013-of-00041.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c3352b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Frances Ellen Watkins Harper House is a hi...\n",
       "1    Uza may refer to:\\nPlaces\\nUza, Landes, a vill...\n",
       "2    Tommy Kirkham is a Northern Ireland loyalist p...\n",
       "3    A cotyledon is a significant part of the embry...\n",
       "4    Det som varit ÄR is a Viking rock album, relea...\n",
       "5    Middlebury Township may refer to the following...\n",
       "6    The list of North Carolina hurricanes between ...\n",
       "7    The Martyrs of Abitinae (or Abitinian Martyrs)...\n",
       "8    The Fabyan Windmill is an authentic, working D...\n",
       "9    HLA-B54 (B54) is an HLA-B serotype. B54 is a s...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a864e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f7e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "2f851bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MinHash:\n",
    "    def __init__(self, num_permutations: int):\n",
    "        self.num_permutations = num_permutations\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        return re.sub(\"( )+|(\\n)+\",\" \",text).lower()\n",
    "\n",
    "    def tokenize(self, text: str) -> set:\n",
    "        text = self.preprocess_text(text)      \n",
    "        return set(text.split(' '))\n",
    "    \n",
    "    def get_occurrence_matrix(self, corpus_of_texts: list[set]) -> pd.DataFrame:\n",
    "        occurrence_dict = {}\n",
    "        for i,x in enumerate(corpus_of_texts):\n",
    "            tokens = self.tokenize(x)\n",
    "            occurrence_dict[i] = {token: 1 for token in tokens}\n",
    "        \n",
    "        return pd.DataFrame(occurrence_dict).fillna(0.0)\n",
    "    \n",
    "    def get_new_index(self, x: int, permutation_index: int) -> int:\n",
    "        ### values_dict - нужен для совпадения результатов теста, а в общем случае используется рандом\n",
    "        values_dict = {\n",
    "            'a': [3, 4, 5, 7, 8],\n",
    "            'b': [3, 4, 5, 7, 8] \n",
    "        }\n",
    "        a = values_dict['a'][permutation_index]\n",
    "        b = values_dict['b'][permutation_index]\n",
    "        return (a*(x+1) + b) % 23 ### здесь важно, чтобы число было >= rows_number и было боижайшим простым числом к rows_number\n",
    "    \n",
    "    \n",
    "    def _get_jaccard_similarity(self, set_a: set, set_b: set) -> float:\n",
    "        intersection = set_a & set_b\n",
    "        union = set_a | set_b\n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def get_jaccard_similarity(self, min_hash_matrix) -> float:\n",
    "        jaccard_matrix = np.zeros((min_hash_matrix.shape[1], min_hash_matrix.shape[1]))\n",
    "        for i in range(0, min_hash_matrix.shape[1]):\n",
    "            for j in range(i+1, min_hash_matrix.shape[1]):\n",
    "                jaccard_matrix[i][j] = self._get_jaccard_similarity(set(ar[::,i]), set(ar[::,j]))\n",
    "        return jaccard_matrix\n",
    "     \n",
    "    \n",
    "    def get_minhash(self, occurrence_matrix: pd.DataFrame) -> np.array:\n",
    "        min_hash_matrix = np.zeros((self.num_permutations, len(occurrence_matrix.columns)))\n",
    "        num_docs = len(occurrence_matrix.columns)\n",
    "        docs_names = occurrence_matrix.columns\n",
    "        for num in range(self.num_permutations):\n",
    "            occurrence_matrix[f'new_index_{i}'] = [self.get_new_index(index, permutation_index=num) for index in range(len(occurrence_matrix))]\n",
    "            for doc_id, doc_name in enumerate(docs_names):\n",
    "                min_hash_matrix[num][doc_id] = occurrence_matrix[occurrence_matrix[doc_id] == 1][f'new_index_{i}'].min()\n",
    "            \n",
    "        return min_hash_matrix\n",
    "    \n",
    "    def run_minhash(self,  corpus_of_texts: list[str]):\n",
    "        occurrence_matrix = self.get_occurrence_matrix(corpus_of_texts)\n",
    "        minhash = self.get_minhash(occurrence_matrix)\n",
    "        jaccard_matrix = self.get_jaccard_similarity(minhash)\n",
    "        return jaccard_matrix\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "682425f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHash(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8db6b9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я люблю Париж\n",
      "я не люблю Париж\n",
      "я люблю Рим\n",
      "         0    1    2\n",
      "люблю  1.0  1.0  1.0\n",
      "париж  1.0  1.0  0.0\n",
      "я      1.0  1.0  1.0\n",
      "не     0.0  1.0  0.0\n",
      "рим    0.0  0.0  1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = minhash.run(Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "141c9b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.33333333],\n",
       "       [0.        , 0.        , 0.33333333],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "60525bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f2b839c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = ['я люблю Париж', \n",
    "'я не люблю Париж' ,\n",
    "'я люблю Рим']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837dbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19e4d1e5",
   "metadata": {},
   "source": [
    "## Алгоритм MinHashLSH. \n",
    "\n",
    "Одна из самых дорогих операций - это попарное сравнения похожести двух объектов. Давайте разобьем матрицу сигнатур на N батчей и будем сравнивать документы внутри небольших батчей. Если сигнатуры документа А и B полностью совпадают - то документы похожи. То есть если совпали хотя бы в одном бакете - документы похожи, если нет - то считаем что нет. Далее к оставшимся документам  уже можно применить полноценный MinHash без батчей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603843b",
   "metadata": {},
   "source": [
    "Возьмем матрицу сигнатур из первого примера:\n",
    "\n",
    "\n",
    "    Doc1 Doc2 Doc3  \n",
    "    [3,   1,    2]  \n",
    "    [2,   1,    2]  \n",
    "    [2,   1,    3] \n",
    "    [1,   1,    1]\n",
    "    \n",
    "    \n",
    "И разобем ее на 2 бакета:\n",
    "\n",
    "\n",
    "\n",
    "    Doc1 Doc2 Doc3  \n",
    "    [3,   1,    2]  b1\n",
    "    [2,   1,    2]  b1\n",
    "    [2,   1,    3]  b2\n",
    "    [1,   1,    1]  b2\n",
    "    \n",
    "    \n",
    "Если сравнивать по бакетам - то кандидатами на похожие являются только документы Doc2, Doc3.\n",
    "\n",
    "\n",
    "В задании для простоты, давайте считать что баскеты будем распределять по порядку как на примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31796d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "cfa16c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinHashLSH(MinHash):\n",
    "    def __init__(self, num_permutations: int, num_buckets: int):\n",
    "        self.num_permutations = num_permutations\n",
    "        self.num_buckets = num_buckets\n",
    "        \n",
    "    def get_buckets(self, minhash: np.array) -> np.array:\n",
    "        size_of_bucket = int(len(minhash)/self.num_buckets)\n",
    "        \n",
    "        if len(minhash) <= size_of_bucket:\n",
    "            return np.array([minhash])\n",
    "        \n",
    "        res_array = []\n",
    "        for i in range(min(self.num_buckets, len(minhash))):\n",
    "            res_array.append(minhash[i*size_of_bucket:(i+1)*size_of_bucket])\n",
    "        \n",
    "        return np.array(res_array)\n",
    "    \n",
    "    def get_similar_candidates(self, buckets) -> list[tuple]:\n",
    "        similar_candidates = []\n",
    "        for bucket in buckets:\n",
    "            for i in range(bucket.shape[1]):\n",
    "                for j in range(i+1, bucket.shape[1]):\n",
    "                    if (bucket[::, i] == bucket[::, j]).all():\n",
    "                        similar_candidates.append((i,j))\n",
    "        return similar_candidates\n",
    "        \n",
    "    def run_minhash_lsh(self, corpus_of_texts: list[str]) -> list[tuple]:\n",
    "        occurrence_matrix = self.get_occurrence_matrix(corpus_of_texts)\n",
    "        minhash = self.get_minhash(occurrence_matrix)\n",
    "        buckets = self.get_buckets(minhash)\n",
    "        similar_candidates = self.get_similar_candidates(buckets)\n",
    "        \n",
    "        return set(similar_candidates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "2881bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hash_lsh = MinHashLSH(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5e514721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1), (0, 2), (1, 2)}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_hash_lsh.run_minhash_lsh(Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fdb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fd63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed2543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e97985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d764d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbdb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e348b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140d032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
